{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ансамблевая классификация\n",
    "Исходный код: https://github.com/ellkrauze/decision_tree_classification\n",
    "## Задание\n",
    "1. Разработайте программу, которая выполняет классификацию заданного набора данных с помощью одной из техник ансамблевой классификации. Параметрами программы являются набор данных, ансамблевая техника (бэггинг, случайный лес или бустинг), количество участников ансамбля, а также параметры в соответствии с выбранной техникой ансамблевой классификации.\n",
    "2. Проведите эксперименты на наборах данных из задания Классификация с помощью дерева решений, варьируя количество участников ансамбля (от 50 до 100 с шагом 10).\n",
    "3. Выполните визуализацию полученных результатов в виде следующих диаграмм:\n",
    "показатели качества классификации в зависимости от количества участников ансамбля для заданного набора данных; нанесите на диаграмму соответствующие значения, полученные в задании Классификация с помощью дерева решений.\n",
    "## Теория\n",
    "**Целью ансамблевых методов** является объединение прогнозов нескольких базовых оценок, построенных с использованием заданного алгоритма обучения, для улучшения обобщаемости/надежности по сравнению с одной оценкой.\n",
    "\n",
    "Обычно выделяют два семейства ансамблевых методов:\n",
    "\n",
    "1. В **методах усреднения** основным принципом является независимое построение нескольких оценщиков, а затем усреднение их прогнозов. В среднем комбинированная оценка обычно лучше, чем любая из оценок с одной базой, потому что ее дисперсия уменьшается.\n",
    "Примеры: Bagging methods, Forests of randomized trees…\n",
    "\n",
    "2. Напротив, в **методах бустинга** базовые оценки строятся последовательно, и каждый пытается уменьшить смещение комбинированной оценки. Мотивация состоит в том, чтобы объединить несколько слабых моделей для создания мощного ансамбля.\n",
    "Примеры: AdaBoost, Gradient Tree Boosting…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import category_encoders as ce\n",
    "import sklearn.metrics \n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train and test data \n",
    "# based on given test size\n",
    "# Parameters:\n",
    "# - df is given dataset\n",
    "# - class_col is the label column\n",
    "# - test_size is the proportion of the dataset to include in the train split\n",
    "def get_splitted_train_test(df, class_col, test_size):\n",
    "    # Declare feature vector and target variable \n",
    "    X = df.drop([class_col], axis=1)\n",
    "    y = df[class_col]\n",
    "    # Split data into separate training and test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size, random_state = 42)\n",
    "    return (X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoded_X_data(X_train, X_test):\n",
    "    # encode variables with ordinal encoding\n",
    "    cols = X_train.columns.values.tolist()\n",
    "    encoder = ce.OrdinalEncoder(cols=cols)\n",
    "    X_train = encoder.fit_transform(X_train)\n",
    "    X_test = encoder.transform(X_test)\n",
    "    return (X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, X_train, X_test, y_train, y_test):\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    y_train_pred = model.predict(X_train)\n",
    "\n",
    "    print(\"TRAINIG RESULTS: \\n===============================\")\n",
    "    clf_report = pd.DataFrame(classification_report(y_train, y_train_pred, output_dict=True, zero_division=True))\n",
    "    print(f\"CONFUSION MATRIX:\\n{confusion_matrix(y_train, y_train_pred)}\")\n",
    "    print(f\"ACCURACY SCORE:\\n{accuracy_score(y_train, y_train_pred):.4f}\")\n",
    "    print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")\n",
    "\n",
    "    print(\"TESTING RESULTS: \\n===============================\")\n",
    "    clf_report = pd.DataFrame(classification_report(y_test, y_test_pred, output_dict=True, zero_division=True))\n",
    "    print(f\"CONFUSION MATRIX:\\n{confusion_matrix(y_test, y_test_pred)}\")\n",
    "    print(f\"ACCURACY SCORE:\\n{accuracy_score(y_test, y_test_pred):.4f}\")\n",
    "    print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier\n",
    "\n",
    "def ensemble_methods(method, n_estimators = 50):\n",
    "    model = DecisionTreeClassifier()\n",
    "    if method == 'BaggingClassifier':\n",
    "        clf = BaggingClassifier(base_estimator=model, \n",
    "                                n_estimators=n_estimators, \n",
    "                                random_state=42)\n",
    "    elif method == 'RandomForestClassifier':\n",
    "        clf = RandomForestClassifier(n_estimators=n_estimators, \n",
    "                                    random_state=42)\n",
    "    elif method == 'AdaBoostClassifier':\n",
    "        clf = AdaBoostClassifier(n_estimators=n_estimators)\n",
    "    else:\n",
    "        return\n",
    "    return clf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(dataset, label, model, test_size_arr):\n",
    "    accuracy_arr = []\n",
    "    precision_arr = []\n",
    "    recall_arr = []\n",
    "    f1_arr = []\n",
    "    for test_size in test_size_arr:\n",
    "        # Split train and test data\n",
    "        X_train, X_test, y_train, y_test= get_splitted_train_test(dataset, label, test_size)\n",
    "\n",
    "        # Encode categorical variables\n",
    "        X_train, X_test = get_encoded_X_data(X_train, X_test)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)  \n",
    "        \n",
    "        # Save metrics\n",
    "        accuracy_arr.append(sklearn.metrics.accuracy_score(y_test, y_pred))\n",
    "        precision_arr.append(sklearn.metrics.precision_score(y_test, y_pred, average='weighted', zero_division=1))\n",
    "        recall_arr.append(sklearn.metrics.recall_score(y_test, y_pred, average='weighted', zero_division=1))\n",
    "        f1_arr.append(sklearn.metrics.f1_score(y_test, y_pred, average='weighted'))\n",
    "\n",
    "    return accuracy_arr, precision_arr, recall_arr, f1_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_list(name, arr):\n",
    "    print(name)\n",
    "    for i in range(len(arr)):\n",
    "        print(f\"{arr[i]}\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Census Income dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lerus\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\util\\_decorators.py:311: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  return func(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "DATASET_PATH = os.path.join('D:\\projects\\decision_tree_classification\\datasets','adult.data')\n",
    "label = \"income\" # label\n",
    "dataset = pd.read_csv(DATASET_PATH, sep=\", \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grades dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = os.path.join('D:\\projects\\decision_tree_classification\\datasets','grades.csv')\n",
    "label = \"GRADE\"\n",
    "dataset = pd.read_csv(DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRITERION='gini'\n",
    "# CRITERION='entropy'\n",
    "TEST_SIZE=0.3\n",
    "# TEST_SIZE_ARR = [0.4, 0.3, 0.2, 0.1]\n",
    "TEST_SIZE_ARR = [0.3]\n",
    "methods = ['BaggingClassifier', 'RandomForestClassifier', 'AdaBoostClassifier']\n",
    "# N_ESTIMATORS=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset = D:\\projects\\decision_tree_classification\\datasets\\grades.csv\n",
      "METHOD = BaggingClassifier\n",
      "N_ESTIMATORS,ACCURACY,PRECISION,RECALL,F1-SCORE\n",
      "50,0.3181818181818182,0.4777777777777778,0.3181818181818182,0.27171717171717175\n",
      "60,0.3181818181818182,0.4777777777777778,0.3181818181818182,0.27171717171717175\n",
      "70,0.3181818181818182,0.4777777777777778,0.3181818181818182,0.27171717171717175\n",
      "80,0.3181818181818182,0.4777777777777778,0.3181818181818182,0.27171717171717175\n",
      "90,0.3181818181818182,0.4777777777777778,0.3181818181818182,0.27171717171717175\n",
      "100,0.3181818181818182,0.4777777777777778,0.3181818181818182,0.27171717171717175\n",
      "Dataset = D:\\projects\\decision_tree_classification\\datasets\\grades.csv\n",
      "METHOD = RandomForestClassifier\n",
      "N_ESTIMATORS,ACCURACY,PRECISION,RECALL,F1-SCORE\n",
      "50,0.2727272727272727,0.3162878787878788,0.2727272727272727,0.24300144300144302\n",
      "60,0.3181818181818182,0.37077922077922076,0.3181818181818182,0.22742030696576154\n",
      "70,0.3181818181818182,0.3996212121212121,0.3181818181818182,0.25707070707070706\n",
      "80,0.3181818181818182,0.3996212121212121,0.3181818181818182,0.25707070707070706\n",
      "90,0.3181818181818182,0.3996212121212121,0.3181818181818182,0.25707070707070706\n",
      "100,0.3181818181818182,0.3996212121212121,0.3181818181818182,0.25707070707070706\n",
      "Dataset = D:\\projects\\decision_tree_classification\\datasets\\grades.csv\n",
      "METHOD = AdaBoostClassifier\n",
      "N_ESTIMATORS,ACCURACY,PRECISION,RECALL,F1-SCORE\n",
      "50,0.13636363636363635,0.30656565656565654,0.13636363636363635,0.09081726354453627\n",
      "60,0.13636363636363635,0.30656565656565654,0.13636363636363635,0.09081726354453627\n",
      "70,0.13636363636363635,0.30656565656565654,0.13636363636363635,0.09081726354453627\n",
      "80,0.13636363636363635,0.30656565656565654,0.13636363636363635,0.09081726354453627\n",
      "90,0.13636363636363635,0.30656565656565654,0.13636363636363635,0.09081726354453627\n",
      "100,0.13636363636363635,0.30656565656565654,0.13636363636363635,0.09081726354453627\n"
     ]
    }
   ],
   "source": [
    "for METHOD in methods:\n",
    "    print(f'Dataset = {DATASET_PATH}')\n",
    "    print(f'METHOD = {METHOD}')\n",
    "    print('N_ESTIMATORS,ACCURACY,PRECISION,RECALL,F1-SCORE')\n",
    "    for N_ESTIMATORS in range(50,110,10):  \n",
    "        classifier = ensemble_methods(method = METHOD, n_estimators = N_ESTIMATORS)\n",
    "        accuracy_arr, precision_arr, recall_arr, f1_arr = get_metrics(dataset, label, model = classifier, test_size_arr = TEST_SIZE_ARR)\n",
    "        \n",
    "        print(f'{N_ESTIMATORS},{accuracy_arr[0]},{precision_arr[0]},{recall_arr[0]},{f1_arr[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = ensemble_methods(method = METHOD, n_estimators = N_ESTIMATORS)\n",
    "accuracy_arr, precision_arr, recall_arr, f1_arr = get_metrics(dataset, label, model = classifier, test_size_arr = TEST_SIZE_ARR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_size\n",
      "0.4\n",
      "0.3\n",
      "0.2\n",
      "0.1\n",
      "accuracy\n",
      "0.8272552783109405\n",
      "0.8394922714709796\n",
      "0.8367879625364655\n",
      "0.8345102855388394\n",
      "precision\n",
      "0.8186485278283226\n",
      "0.8336492014378727\n",
      "0.8328129532175675\n",
      "0.8318479641756297\n",
      "recall\n",
      "0.8272552783109405\n",
      "0.8394922714709796\n",
      "0.8367879625364655\n",
      "0.8345102855388394\n",
      "f1-score\n",
      "0.8208442574207844\n",
      "0.8356292377115739\n",
      "0.8344260292650884\n",
      "0.833021920430877\n"
     ]
    }
   ],
   "source": [
    "display_list(\"test_size\", TEST_SIZE_ARR)\n",
    "display_list(\"accuracy\", accuracy_arr)\n",
    "display_list(\"precision\", precision_arr)\n",
    "display_list(\"recall\", recall_arr)\n",
    "display_list(\"f1-score\", f1_arr)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dcf54b044ca9e8fc45e0b43406de43bea0ad89c58ce62bc925fe193d2238dfa4"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
