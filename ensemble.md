# Ансамблевая классификация
**Ансамблевые методы** — это парадигма машинного обучения, где несколько моделей (часто называемых «слабыми учениками») обучаются для решения одной и той же проблемы и объединяются для получения лучших результатов. Основная гипотеза состоит в том, что при правильном сочетании слабых моделей мы можем получить более точные и/или надежные модели.
## Эксперименты
Проведены эксперименты на наборах данных из задания [Классификация с помощью дерева решений](/README.md),количество участников ансамбля варьировалось от 50 до 100 с шагом 10. Размер тестовой выборки был постоянным - 30%.

## Grades Dataset
### Bagging Classifier
В параллельных методах мы рассматриваем разных учеников независимо друг от друга друга и, таким образом, можно обучать их одновременно. Наиболее известным из таких подходов является **бэггинг** (от англ. «bootstrap aggregation»), целью которого является создание ансамблевой модели, которая является более надежной, чем отдельные модели, ее составляющие.
![Classifier](/media/grades/baggingclf.jpg)
#### Вывод
Показатели качества accuracy, precision и recall модели дерева решений на 43% ниже показателей качества классификатора Bagging. Показатель f-score ниже на 4%.

### Random Forest Classifier
**Случайный лес** представляет собой метод бэггинга, где глубокие деревья, обученные на бутстрап выборках, объединяются для получения результата с более низким разбросом. Тем не менее, случайные леса также используют другой прием, чтобы несколько обученных деревьев были менее коррелированными друг с другом: при построении каждого дерева вместо выбора всех признаков из датасета для генерации бутстрэпа мы выбираем и сохраняем только случайное их подмножество для построения дерева (обычно одинаковое для всех бутстрэп выборок).
![Classifier](/media/grades/randomforestclf.jpg)
#### Вывод
Показатели качества accuracy, precision и recall модели дерева решений на 41%, 28% и 41% ниже показателей качества классификатора Random Forest, соответственно. Показатель f-score ниже на 3%.

### AdaBoost Classifier
Методы бустинга работают в том же духе, что и методы бэггинга: мы создаем семейство моделей, которые объединяются, чтобы получить сильного ученика, который лучше работает. Однако, в отличие от бэггинга, которое в основном направлено на уменьшение разброса, бустинг — это метод, который заключается в том, чтобы адаптировать последовательно нескольких слабых учеников адаптивным способом: каждая модель в последовательности подбирается, что придает большее значение объектам в датасете, которые плохо обрабатывались предыдущими моделями в последовательности. Адаптивный бустинг обновляет веса, прикрепленные к каждому из объектов обучающего датасета.
![Classifier](/media/grades/adaboostclf.jpg)

#### Вывод
Показатели качества accuracy и recall модели дерева решений на 33% выше показателя качества классификатора AdaBoost. Показатель f-score не изменился. Показатель precision ниже на 12%.

## Census Income Dataset
![Classifier](/media/adult/baggingclf.jpg)
![Classifier](/media/adult/randomforestclf.jpg)
![Classifier](/media/adult/adaboostclf.jpg)

#### Вывод
Показатели качества accuracy, precision, recall и f-score модели дерева решений в среднем на 3-5% ниже показателей качества классификаторов.